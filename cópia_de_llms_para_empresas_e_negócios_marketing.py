# -*- coding: utf-8 -*-

# Projeto para Departamento de Marketing

Nosso objetivo com esse projeto √© criar um assistente de gera√ß√£o de conte√∫do automatizado, que adapta o texto ao p√∫blico e ao canal de divulga√ß√£o. Confira os slides para mais detalhes sobre a proposta desse estudo de caso.


> **Importante:** Caso d√™ algum erro no processo de instala√ß√£o e que impe√ßa de prosseguir com a execu√ß√£o do c√≥digo, confira o Colab da aula e verifique se fez uma c√≥pia do mais atualizado, pois atualizaremos essas etapas de instala√ß√£o com os comandos atualizados (caso seja necess√°ria alguma mudan√ßa no comando de instala√ß√£o).

Vamos usar primeiro o ipynb no Colab para desenvolver e validar a l√≥gica com LLMs, onde aprenderemos a deixar uma aplica√ß√£o funcional dentro do pr√≥prio Colab usando ipywidgets. Ao final, veremos como adaptar isso para uma interface profissional usando o framework Streamlit, pronto para publicar. Isso evita retrabalho, ajuda a testar ideias com rapidez e foca primeiro no que importa: o n√∫cleo funcional, a l√≥gica e conceitos.

## Instala√ß√£o das bibliotecas

Abaixo instalaremos algumas bibliotecas essenciais para o desenvolvimento de nosso projeto.

Para instala√ß√£o usaremos o comando pip install. Passaremos o par√¢metro -q (quiet) para reduzir a verbosidade da sa√≠da no terminal, exibindo apenas erros e mensagens essenciais. √â usado para simplificar a visualiza√ß√£o durante instala√ß√µes automatizadas ou em ambientes onde logs detalhados n√£o s√£o necess√°rios.
"""

!pip install -q "requests==2.32.4" langchain-community langchain-groq ipywidgets

"""### Importar bibliotecas  

"""

from google.colab import widgets
import ipywidgets as widgets

from langchain_groq import ChatGroq
import os
import getpass

"""# Cria√ß√£o dos campos - Interface

Antes de partirmos para o c√≥digo, √© importante definirmos com clareza os campos que a aplica√ß√£o ir√° utilizar. Essa etapa √© essencial para evitar dispers√£o e garantir que o desenvolvimento seja focado nas necessidades reais da empresa.

> Campos *(conforme discutido na apresenta√ß√£o do estudo de caso)*
 * Plataforma de destino (ex: Blog, Instagram, LinkedIn, E-mail)
 * Tom da mensagem (ex: Informativo, Inspirador, Urgente, Informal)
 * Comprimento do texto (ex: Curto, M√©dio, Longo)
 * Tema ou t√≥pico (ex: alimenta√ß√£o, sa√∫de mental, exames de rotina, cuidados, etc.)
 * P√∫blico-alvo (Jovens adultos, Fam√≠lias, Idosos, Geral, etc.)
 * Op√ß√µes adicionais:
  * Incluir chamada para a√ß√£o (ex: ‚ÄúAgendar consulta‚Äù ou ‚ÄúConverse com um especialista‚Äù)
  * Retornar hashtags
  * Inserir palavras-chave para incluir no meio do texto


Vamos come√ßar criando um campo em formato de texto. O `widgets.Text` cria um campo livre para digita√ß√£o, onde o usu√°rio insere o conte√∫do manualmente.

* `description`: texto que aparece como r√≥tulo do campo (ajuda a identificar sua fun√ß√£o).
* `placeholder`: texto que aparece dentro do campo antes do preenchimento, como sugest√£o ou exemplo. Vamos aproveitar para colocar uma sugest√£o j√° do que o usu√°rio pode digitar, o que √© uma boa pr√°tica de user experience (UX)

Obs: Pensando nas boas pr√°ticas, tamb√©m vamos aproveitar para definir os nomes das vari√°veis em ingl√™s (tema vai ser *topic*, p√∫blico-alvo vai ser *audience*, etc.)


"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.'
)

"""### Exibindo o widget

Para exibir os campos/widgets que criamos vamos usar o m√©todo display(). Com isso o campo vai aparecer dentro da sa√≠da do bloco de c√≥digo abaixo, assim exibindo tudo de forma interativa dentro desse notebook.
"""

display(topic)

topic.value

"""### Ajustando propriedades do campo

Por padr√£o, o widget Text do ipywidgets cria um campo de entrada relativamente estreito, o que pode n√£o ser ideal quando esperamos que o usu√°rio digite frases ou trechos mais longos.

Com `layout=widgets.Layout(width='500px')` definimos explicitamente a largura do campo como 500 pixels, o que √© mais apropriado quando esperamos frases completas.

* Voc√™ pode ajustar esse valor conforme a necessidade - ex: '100%' para ocupar toda a largura do container (deixando responsivo), ou '700px' para um campo ainda maior.

"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.',
    layout = widgets.Layout(width='500px')
)
display(topic)

"""### Outros formatos de campos

Para adicionar campos de sele√ß√£o pr√°ticos e din√¢micos √† nossa aplica√ß√£o, utilizaremos a fun√ß√£o widgets.Dropdown, que exibe op√ß√µes em formato de lista suspensa. Passaremos as escolhas dispon√≠veis atrav√©s do par√¢metro options e, para otimizar a interface e facilitar futuras altera√ß√µes, definiremos uma largura padr√£o para esses campos usando uma vari√°vel, permitindo ajustes globais de tamanho de forma simples, o que pode ser muito √∫til caso os valores pr√©-definidos sejam extensos.

"""

w_dropdown = '250px'

platform = widgets.Dropdown(
    options = ['Instagram', 'Facebook', 'LinkedIn', 'Blog', 'E-mail'],
    description = 'Plataforma',
    layout = widgets.Layout(width = w_dropdown)
)

tone = widgets.Dropdown(
    options=['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'],
    description='Tom:',
    layout=widgets.Layout(width=w_dropdown)
)

length = widgets.Dropdown(
    options=['Curto', 'M√©dio', 'Longo'],
    description='Tamanho:',
    layout=widgets.Layout(width=w_dropdown)
)

audience = widgets.Dropdown(
    options=['Geral', 'Jovens adultos', 'Fam√≠lias', 'Idosos', 'Adolescentes'],
    description='P√∫blico-alvo:',
    layout=widgets.Layout(width=w_dropdown)
)


display(platform, tone, length, audience)

platform.value

"""Para incorporar op√ß√µes de ativar/desativar funcionalidades, como incluir uma Chamada para A√ß√£o (CTA) ou solicitar sugest√µes de hashtags, utilizaremos widgets.Checkbox.

Estes campos booleanos (Verdadeiro/Falso) ser√£o configurados com um valor inicial (por padr√£o, desmarcado) e uma descri√ß√£o clara de sua fun√ß√£o, permitindo ao usu√°rio controlar facilmente aspectos espec√≠ficos da gera√ß√£o de conte√∫do.
"""

cta = widgets.Checkbox(
    value = False,
    description = 'Incluir CTA'
)

hashtags = widgets.Checkbox(
    value=False,
    description='Retornar Hashtags',
)

display(cta)

cta.value

"""Para permitir a inser√ß√£o de textos mais longos, como listas de palavras-chave para SEO, implementaremos um campo do tipo Textarea. Este campo opcional dar√° ao usu√°rio a flexibilidade de especificar termos que a IA deve incorporar naturalmente ao conte√∫do, e seu tamanho pode ser ajustado em largura e altura para melhor acomodar o texto inserido, utilizando `widgets.Layout` para definir dimens√µes como height."""

keywords = widgets.Textarea(
    description = 'Palavras-chave (SEO)',
    placeholder = 'Ex: bem-estar, medicina preventiva...',
    layout = widgets.Layout(width = '500px', height = '50px')
)

display(keywords)

"""## Criando o bot√£o de gera√ß√£o

Vamos agora adicionar um bot√£o √† interface. Esse bot√£o ser√° clicado para gerar o conte√∫do com base nos campos preenchidos. O par√¢metro description aqui √© o texto que aparece no bot√£o.

"""

generate_button = widgets.Button(
    description = 'Gerar conte√∫do',
)

display(generate_button)

"""## Exibi√ß√£o do resultado

Precisamos criar um espa√ßo para exibir o output, que √© o resultado gerado pela LLM.

Usamos o Output() para mostrar o resultado da gera√ß√£o de conte√∫do. Ele cria uma ‚Äú√°rea de resposta‚Äù, onde vamos exibir o conte√∫do gerado. Tudo que for mostrado com display() ou print() dentro dele aparecer√° aqui.

"""

output = widgets.Output()

"""### Definindo a√ß√£o do bot√£o

Por enquanto o bot√£o n√£o faz nada, precisamos criar uma fun√ß√£o que ser√° executada quando ele for clicado.

Explicando os par√¢metros:

* `b` √© o pr√≥prio bot√£o sendo passado como argumento (padr√£o do on_click).

* `with output`: garante que tudo dentro desse bloco apare√ßa na √°rea de sa√≠da.

* `clear_output()` limpa o resultado anterior, evitando sobreposi√ß√£o de textos.

"""

def generate_result(b):
  with output:
    output.clear_output()
    print("Ok!")

"""**Ligando o bot√£o √† fun√ß√£o**

`.on_click()` define que nossa fun√ß√£o ser√° executada quando o bot√£o for pressionado
"""

generate_button.on_click(generate_result)

"""**Testando**

Execute o bloco de c√≥digo abaixo e clique no bot√£o para verificar se nosso m√©todo est√° funcionando.
"""

display(generate_button, output)

"""## Exibindo os campos juntos na interface

Por fim, precisamos organizar os campos e exibi-los num layout final junto ao bot√£o.
Antes de chamarmos a fun√ß√£o display (para exibir tudo de forma interativa dentro desse notebook) vamos usar a fun√ß√£o `VBox()`, para organizar os elementos na vertical, na ordem em que forem listados.

E para evitar a repeti√ß√£o, coloque dentro de uma fun√ß√£o chamada "create_form", que retorne esse VBox com os widgets. Assim seu c√≥digo fica mais limpo e reutiliz√°vel, pois usaremos mais tarde esses campos novamente
"""

def create_form():
  return widgets.VBox([
      topic,
      platform,
      tone,
      length,
      audience,
      cta,
      hashtags,
      keywords,
      generate_button,
      output
  ])

form = create_form()

display(form)

"""# Conectando com a LLM

Para integrar a LLM √† nossa aplica√ß√£o, precisamos definir o modelo e a forma de implementa√ß√£o, que pode ser via download (para modelos open source, garantindo execu√ß√£o local e privacidade) ou atrav√©s de API (simplificando a integra√ß√£o, oferecendo boa performance em qualquer m√°quina, mas com processamento de dados em servidores externos).

## Escolhendo o modelo

Na fase inicial de testes, recomenda-se come√ßar com um modelo open source acess√≠vel via API gratuita, o que simplifica a implementa√ß√£o e reduz custos. Mesmo ap√≥s a aplica√ß√£o estar funcionando, esses modelos seguem vantajosos pela flexibilidade e economia. Neste curso, iniciaremos com o uso via API para evitar a complexidade da configura√ß√£o local. Mais adiante, ensinaremos como rodar modelos localmente, permitindo que voc√™ compare as abordagens e escolha a mais adequada ao seu caso.

Usaremos a biblioteca LangChain para integrar com a Groq, aproveitando seu m√≥dulo nativo de conex√£o e os benef√≠cios que ela oferece no desenvolvimento.

Para escolher bons modelos, recomendamos consultar leaderboards comparativos, como:
 * o https://lmarena.ai/?leaderboard
 * ou ranking espec√≠fico para portugu√™s na Hugging Face - https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard

**Adicionando a key**

Antes de come√ßar com o c√≥digo, voc√™ deve colar no campo a sua key gerada dentro do painel do Groq: https://console.groq.com/keys
"""

os.environ["GROQ_API_KEY"] = getpass.getpass()

"""Lembre-se que n√£o precisamos pagar para usar modelos disponibilizados gratuitamente pelo provedor.

* Essa pr√≥xima linha usa o m√©todo ChatGroq para configura√ß√£o do modelo via API do Groq.

* Escolhemos um modelo gratuito, dentro da aba *free tier* https://console.groq.com/docs/rate-limits. Copie o ID do modelo e adicione no campo a seguir


"""

id_model = "llama-3.1-8b-instant" #@param {type: "string"}

llm = ChatGroq(
    model = id_model,
    temperature = 0.7,
    max_tokens=None,
    timeout = None,
    max_retries = 2,
)

"""### Explica√ß√µes do m√©todo

Para este teste definimos a temperatura como 0.7.
A temperatura √© um hiperpar√¢metro que ajusta a aleatoriedade da resposta da LLM.

* Temperaturas mais altas (0.8-1.0) geram sa√≠das mais criativas, ideais para brainstorming, enquanto temperaturas baixas (0.0-0.4) produzem respostas mais focadas e determin√≠sticas, adequadas para tarefas t√©cnicas;
* valores m√©dios (0.5-0.7) oferecem um equil√≠brio, sendo um bom ponto de partida para gera√ß√£o de conte√∫do geral, embora seja recomend√°vel experimentar diferentes valores conforme o objetivo e o modelo.
Al√©m da temperatura, outros par√¢metros como max_tokens (limite de tokens), timeout (tempo m√°ximo de resposta) e max_retries (tentativas em caso de falha) podem ser configurados para otimizar o comportamento da LLM, com a documenta√ß√£o da LangChain para Groq oferecendo detalhes sobre todas as op√ß√µes dispon√≠veis.

https://python.langchain.com/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html

### Formato das mensagens

Ao interagir com a LLM, estruturamos o prompt como uma troca de mensagens, cada uma com uma fun√ß√£o (ou *role*, como "human" para nossa entrada e "system" para instru√ß√µes gerais que garantem consist√™ncia) e um conte√∫do (a mensagem em si, seja texto ou dados estruturados).

O prompt de sistema √© crucial para definir o comportamento base da LLM, como atribuir um papel ou instru√ß√µes padr√£o, e embora um prompt gen√©rico possa funcionar, um prompt de sistema espec√≠fico para a aplica√ß√£o melhora significativamente a consist√™ncia dos resultados.
"""

prompt = "Ol√°! Quem √© voc√™?" # @param {type:"string"}

template = [
    ("system", "Voc√™ √© um redator profissional."),
    ("human", prompt)
]

res = llm.invoke(template)
res

prompt = "Ol√°! Quem √© voc√™?" # @param {type:"string"}

template = [
    ("system", "Voc√™ √© um redator profissional."),
    ("human", prompt)
]

res = llm.invoke(template)
res

res.content

"""**Usando com m√©todo de template do LangChain**

Para criar prompts din√¢micos e organizados, especialmente em aplica√ß√µes maiores e reutiliz√°veis com LangChain, utilizamos `ChatPromptTemplate.from_messages()`, que permite inserir de forma organizada vari√°veis (como {input}) e separar a l√≥gica do prompt, tornando o c√≥digo mais limpo e escal√°vel.

Em vez de invocar a LLM diretamente, criamos uma "chain" que combina este template de prompt com o modelo.

> Para contextualizar, o que s√£o **chains**: Chain do LangChain (Corrente, Cadeias ou ainda Sequencias) √© uma composi√ß√£o de etapas que processam dados em sequ√™ncia ‚Äî aqui, a entrada √© formatada pelo prompt e enviada ao modelo. A vantagem √© que chains permitem combinar v√°rias a√ß√µes (como formatar, gerar, filtrar, armazenar) de forma modular e reutiliz√°vel, facilitando aplica√ß√µes mais robustas. Elas funcionam ao encadear componentes, onde a sa√≠da de um se torna a entrada do pr√≥ximo, criando uma sequ√™ncia l√≥gica de opera√ß√µes.
"""

from langchain_core.prompts import ChatPromptTemplate

prompt = "Ol√°! quem √© voc√™?"  # @param {type:"string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um redator profissional"),
    ("human", "{prompt}")
])

chain = template | llm

res = chain.invoke({"prompt": prompt})
res.content

"""### Estendendo a chain / Output parser

Para trabalhar com a sa√≠da de sequ√™ncia "crua" da mensagem, Langchain oferece "Output Parsers", como o StrOutputParser, que processa a sa√≠da do modelo em um formato mais acess√≠vel, convertendo-a em uma string.

Se o modelo (LLM) j√° produz uma string, o StrOutputParser simplesmente a repassa; se for um ChatModel que produz uma mensagem, ele extrai o conte√∫do do atributo `.content`. Embora res.content possa ser usado diretamente no caso de modelos LLM que j√° retornam string, incluir o StrOutputParser na chain √© uma boa pr√°tica para obter o valor string diretamente, tornando-se especialmente √∫til ao integrar ChatModels.


"""

from langchain_core.output_parsers import StrOutputParser

prompt = "Ol√°! quem √© voc√™?"  # @param {type:"string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um redator profissional"),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
res

"""## Melhorando a exibi√ß√£o do resultado

Note acima que o resultado n√£o ficou t√£o apresent√°vel no Colab, podemos melhorar a sua visualiza√ß√£o usando **Markdown**.
* Markdown √© uma linguagem de marca√ß√£o simples e leve que facilita a formata√ß√£o de texto usando s√≠mbolos como asteriscos e hashtags, sem precisar de HTML. No Google Colab, ele melhora a organiza√ß√£o e a legibilidade, permitindo destacar textos em *it√°lico* , **negrito** e criar t√≠tulos com #, ## ou ### para diferentes n√≠veis.

* Caso queira explorar mais, aqui est√° um guia da sintaxe: https://www.markdownguide.org/basic-syntax/

"""

def show_res(res):
  from IPython.display import Markdown
  display(Markdown(res))

show_res(res)

"""## Juntando em uma fun√ß√£o

Reunir tudo em uma fun√ß√£o facilita a reutiliza√ß√£o, organiza√ß√£o e manuten√ß√£o do c√≥digo, evitando repeti√ß√µes durante nossos testes. Vamos chamar essa fun√ß√£o no bloco de c√≥digo seguinte

"""

def llm_generate(llm, prompt):
  template = ChatPromptTemplate.from_messages([
      ("system", "Voc√™ √© um redator profissional."),
      ("human", "{prompt}"),
  ])

  chain = template | llm | StrOutputParser()

  res = chain.invoke({"prompt": prompt})
  show_res(res)

prompt = "escreva 5 dicas de sa√∫de"  # @param {type:"string"}

llm_generate(llm, prompt)

"""### Outros Modelos Open Source


 * Modelos dispon√≠veis pelo Groq https://console.groq.com/docs/rate-limits (ver os gratuitos - dentro da aba *free tier*)

 Durante a fase de experimenta√ß√£o √© uma boa ideia testar diferentes modelos.

Ap√≥s validar a nossa solu√ß√£o e fazer os testes iniciais, voc√™ pode optar tamb√©m por modelos pagos e propriet√°rios quando o modelo estiver em produ√ß√£o, j√° que agora n√£o estar√° mais desperdi√ßando alguns centavos de d√≥lar em testes.

### Modelos propriet√°rios (exemplo: ChatGPT da OpenAI)

Recomenda-se iniciar os testes com modelos open source e, ap√≥s a valida√ß√£o, migrar para solu√ß√µes pagas, como ChatGPT (OpenAI) ou Claude (Anthropic), para evitar custos desnecess√°rios durante o desenvolvimento.

Solu√ß√µes pagas oferecem modelos de ponta com alta performance e suporte via API, ideais para robustez e facilidade, mas o custo escala com o uso de tokens (segmentos de texto processados); em contraste, modelos open source, execut√°veis localmente ou em servidores pr√≥prios, proporcionam maior controle, privacidade e custo reduzido para larga escala, exigindo, no entanto, mais conhecimento t√©cnico para configura√ß√£o e manuten√ß√£o.

A decis√£o entre API paga e open source deve considerar o volume de uso esperado e a necessidade de personaliza√ß√£o.

Para testar a implementa√ß√£o, faremos um exemplo com o ChatGPT, lembrando que os custos da OpenAI s√£o baseados em tokens (consulte openai.com/api/pricing/).

A grande vantagem de usar LangChain √© que toda a sintaxe e l√≥gica de chains criadas s√£o reaproveit√°veis, alterando-se apenas a forma como a LLM √© carregada, enquanto o restante da aplica√ß√£o permanece o mesmo.

* Valores: https://openai.com/api/pricing/

> Como gerar uma API key

Para utilizar os modelos da OpenAI, √© necess√°rio obter uma chave de API. Siga as etapas abaixo para gerar a sua:

1. Acesse o site da OpenAI e fa√ßa login na sua conta.
2. Navegue at√© a se√ß√£o de chaves de API e clique em "Criar nova chave secreta" - https://platform.openai.com/api-keys
3. Copie a chave gerada e armazene-a em um local seguro. Importante: nunca compartilhe sua chave

> Conferir o uso https://platform.openai.com/usage
"""

!pip install -q langchain-openai

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# https://platform.openai.com/docs/models
from langchain_openai import ChatOpenAI
chatgpt = ChatOpenAI(model = "gpt-4.1-mini")

chatgpt = ChatOpenAI(
    model="gpt-4.1-mini",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2
)

chain_chatgpt = template | chatgpt

res = chain_chatgpt.invoke({"prompt": "Gere um texto de 2 par√°grafos sobre dicas de sa√∫de"})
show_res(res.content)

"""# Construindo o prompt de aplica√ß√£o

Agora que j√° interagimos com o modelo de forma b√°sica, vamos come√ßar a explorar a **engenharia de prompt**. Isso significa aprender a formular perguntas ou instru√ß√µes de forma clara e espec√≠fica para obter respostas mais precisas e √∫teis.

### Estrutura de um prompt

Existem v√°rias t√©cnicas de engenharia de prompt, onde muitas delas se baseiam em princ√≠pios parecidos. Uma abordagem simples para construir um prompt mais completo √© adicionar a ele alguns 'blocos' (componentes), que no caso seriam:
* Papel (Role) - "quem" ele deve interpretar (mais sobre isso abaixo)
* Tarefa (Task) - tarefa que deve realizar
* Entrada (Input) - informa√ß√£o que pode ser usada como contexto para gerar uma resposta (por exemplo o faturamento mensal de uma empresa, ou um dado espec√≠fico sobre algo ou algu√©m)
* Sa√≠da (Output) - como quer que seja o resultado. Podemos especificar tamb√©m regras, como como tamanho do resultado (medido em quantidade de palavras ou par√°grafos por exemplo)
* Restri√ß√µes (Constraints) - o que queremos evitar na resposta. Por exemplo: "evite jarg√µes ou linguagem muito t√©cnica". "N√£o inclua sua an√°lise ou opini√£o".

### Interpreta√ß√£o de pap√©is - Role Prompting

A t√©cnica de role prompting consiste em instruir o modelo de IA a assumir um papel espec√≠fico, como um especialista em determinada √°rea (o modo mais comum, conhecido como "O Especialista", para obter explica√ß√µes t√©cnicas), uma figura hist√≥rica ou um personagem fict√≠cio, o que influencia significativamente o estilo e o conte√∫do da resposta, mesmo que o restante do prompt seja id√™ntico.

Utilizar frases como "Voc√™ √© um redator" ou "Aja como um historiador" no prompt de sistema s√£o exemplos pr√°ticos dessa t√©cnica, que molda a persona da LLM para gerar resultados mais alinhados com o contexto desejado.
"""

prompt = "fale sobre chocolate em 1 par√°grafo"

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um historiador"),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

"""Aqui vemos ent√£o como o tipo de especialista pode impactar totalmente no resultado. Portanto precisamos pedir algo que seja alinhado ao nosso prop√≥sito (copiamos o mesmo c√≥digo do bloco acima, mudando apenas o prompt do system)"""

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um especialista em marketing digital."),
    ("human", "{prompt}"),
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

"""Ser mais espec√≠fico geralmente √© mais indicado. No contexto da nossa aplica√ß√£o o prompt abaixo funciona melhor porque direciona o modelo n√£o apenas para produzir textos bem escritos, mas com foco estrat√©gico (como convers√£o, engajamento e SEO, que s√£o essenciais em campanhas de marketing)."""

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
    ("human", "{prompt}"),
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Usando exemplos - One-Shot e Few-Shot Prompting

 * Zero-Shot ‚Äì O modelo responde sem exemplos, confiando apenas no treinamento
 * One-Shot ‚Äì Um exemplo √© fornecido para orientar a resposta.
 * Few-Shot ‚Äì V√°rios exemplos ajudam o modelo a reconhecer padr√µes e melhorar a precis√£o.


"""

assunto = 'chocolate'

one_shot = f"""
Exemplo:
T√≠tulo: Voc√™ sabia que beber mais √°gua pode melhorar sua concentra√ß√£o?
Texto: A desidrata√ß√£o leve j√° √© suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidrata√ß√£o #foconasaude

Agora gere um novo texto que fale sobre {assunto}
"""

#print(one_shot)

res = chain.invoke({"prompt": one_shot})
show_res(res)

"""O few-shot prompting, ou prompt com exemplos, demonstra √† IA a estrutura, estilo e abordagem desejados para a resposta, como a inclus√£o de hashtags ou a formula√ß√£o de t√≠tulos como perguntas, tornando o processo de instru√ß√£o mais intuitivo e eficiente do que apenas fornecer instru√ß√µes textuais. Embora exemplos possam ser combinados com texto para maior precis√£o, o few-shot prompting com m√∫ltiplos exemplos, como o que veremos a seguir, ajuda a IA a generalizar melhor o padr√£o esperado."""

few_shot = f"""
Exemplo 1:
T√≠tulo: Voc√™ sabia que beber mais √°gua pode melhorar sua concentra√ß√£o?
Texto: A desidrata√ß√£o leve j√° √© suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidrata√ß√£o #foconasaude

Exemplo 2:
T√≠tulo: Comer carboidratos √† noite engorda: Mito ou verdade?
Texto: Esse √© um mito comum. O que realmente importa √© o total cal√≥rico do dia e a qualidade dos alimentos. Com orienta√ß√£o certa, d√° sim para comer bem √† noite sem culpa!
Hashtags: #nutricaosemmitos #equilibrioalimentar

Agora gere um novo texto que fale sobre {assunto}
"""

res = chain.invoke({"prompt": few_shot})
show_res(res)

"""### Guiando o resultado com uma estrutura - Structured Prompting

Para o prompt final de nossa aplica√ß√£o, usaremos tamb√©m o conceito de Prompting estruturado (Structured Prompting), cuja premissa envolve a codifica√ß√£o cuidadosa de instru√ß√µes, exemplos e restri√ß√µes personalizadas para direcionar propositalmente comportamentos de modelos de linguagem para tarefas de um nicho espec√≠ficos.


"""

form = create_form()
display(form)

prompt = f"""
Crie um post para {platform.value} com a seguinte estrutura:
1. Comece com uma pergunta provocativa.
2. Apresente um benef√≠cio claro relacionado ao tema.
3. Finalize com uma chamada para a√ß√£o (CTA) encorajando o leitor a buscar mais informa√ß√µes.

Tema: {topic.value}
P√∫blico-alvo: {audience.value}
Tom: {tone.value}
"""

print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""Para dar mais liberdade √† IA na escolha da estrutura do texto, especialmente considerando que a aplica√ß√£o aceitar√° diversos par√¢metros como plataforma e comprimento, optaremos por um prompt final din√¢mico em vez de um structured prompting r√≠gido, que seria mais adequado para resultados muito espec√≠ficos e poderia levar a publica√ß√µes repetitivas.

### Construindo o prompt final dinamicamente


Este prompt final ser√° constru√≠do a partir das vari√°veis do formul√°rio, organizado em itens leg√≠veis com `-` para f√°cil modifica√ß√£o e escalabilidade.

Cada linha fornecer√° instru√ß√µes claras (canal, tom, p√∫blico...), e op√ß√µes como hashtags ou CTAs ser√£o inclu√≠das condicionalmente usando express√µes inline em Python, adaptando o prompt √†s escolhas do usu√°rio.

Adicionaremos tamb√©m a instru√ß√£o para garantir que a sa√≠da seja limpa e pronta para uso.


"""

prompt = f"""
Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
Retorne em sua resposta apenas o texto final.
- Onde ser√° publicado: {platform.value}.
- Tom: {tone.value}.
- P√∫blico-alvo: {audience.value}.
- Comprimento: {length.value}.
- {"Inclua uma chamada para a√ß√£o clara." if cta.value else "N√£o inclua chamada para a√ß√£o"}
- {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "N√£o inclua hashtags."}
{"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
"""
print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Sobre o prompt e melhorias

N√£o existe um ‚Äúmelhor prompt‚Äù universal ‚Äî o mais eficaz depende sempre do seu objetivo e do contexto da aplica√ß√£o. A melhor forma de descobrir o que funciona √© testando varia√ß√µes e analisando os resultados.

Para encontrar boas alternativas, voc√™ pode:

* Pesquisar por prompt books gratuitos dispon√≠veis na internet

* Usar sites que re√∫nem templates prontos, como PromptHero ou FlowGPT

* Pedir sugest√µes diretamente √† pr√≥pria LLM (‚ÄúComo posso melhorar esse prompt para torn√°-lo mais persuasivo?‚Äù)

* Analisar exemplos de prompts usados em casos reais ou estudos de caso

* Ajustar pequenos trechos do prompt e observar o impacto (tom, foco, estrutura)

* Extra: Combinar t√©cnicas (por exemplo Structured Prompting com few-shot prompting) pode aprimorar ainda mais a qualidade e a relev√¢ncia dos conte√∫dos gerados.

Essas estrat√©gias ajudam a refinar continuamente a performance e alinhar melhor o conte√∫do gerado aos seus objetivos.

# Concluindo a aplica√ß√£o final

Agora que conclu√≠mos a cria√ß√£o do prompt final de nossa aplica√ß√£o, podemos partir para a finaliza√ß√£o.
Precisamos juntar os formul√°rios ao prompt e √† LLM.
"""

def llm_generate(llm, prompt):
  template = ChatPromptTemplate.from_messages([
      ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
      ("human", "{prompt}"),
  ])

  chain = template | llm | StrOutputParser()

  res = chain.invoke({"prompt": prompt})
  return res

def generate_result(b):
  with output:
    output.clear_output()
    prompt = f"""
    Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
    Retorne em sua resposta apenas o texto final e n√£o inclua ela dentro de aspas.
    - Onde ser√° publicado: {platform.value}.
    - Tom: {tone.value}.
    - P√∫blico-alvo: {audience.value}.
    - Comprimento: {length.value}.
    - {"Inclua uma chamada para a√ß√£o clara." if cta.value else "N√£o inclua chamada para a√ß√£o"}
    - {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "N√£o inclua hashtags."}
    {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
    """
    try:
      res = llm_generate(llm, prompt)
      show_res(res)
    except Exception as e:
      print(f"Erro: {e}")

"""Para executar a fun√ß√£o de gera√ß√£o de conte√∫do ao clicar no bot√£o, precisamos primeiro desvincular qualquer callback anterior para evitar execu√ß√µes duplicadas, especialmente em ambientes como o Colab onde o par√¢metro remove=True pode apresentar instabilidades. A solu√ß√£o mais simples e robusta √© redeclarar o output, o generate_button (associando o on_click √† nova fun√ß√£o) e a vari√°vel form chamando create_form(), garantindo uma configura√ß√£o limpa a cada execu√ß√£o da c√©lula."""

output = widgets.Output()
generate_button = widgets.Button(description = "Gerar conte√∫do")
generate_button.on_click(generate_result)
form = create_form()

display(form)

"""**Pronto!** Finalizamos nossa aplica√ß√£o.

Aqui voc√™ pode reunir todo o c√≥digo desenvolvido em um √∫nico bloco, j√° pronto para ser executado e utilizado por quem for operar o sistema.

Para deixar o c√≥digo recolhido por padr√£o, utilize o comando `#@title` no in√≠cio do bloco ‚Äî por exemplo: `#@title Rodar Aplica√ß√£o`
Isso al√©m de criar uma se√ß√£o com t√≠tulo e facilitar a organiza√ß√£o vai permitir que o c√≥digo fique escondido. Para exibir ou ocultar o conte√∫do, basta dar dois cliques sobre o t√≠tulo ("Rodar Aplica√ß√£o").

## Escalando para outras √°reas e adicionando mais campos

Para aumentar a flexibilidade na defini√ß√£o das op√ß√µes dos campos Dropdown, em vez de fix√°-las no c√≥digo, utilizaremos os formul√°rios do Colab com a anota√ß√£o `@param {type:"string"}`. Isso permite que o usu√°rio insira uma lista de valores separados por v√≠rgula diretamente em um campo ao lado da c√©lula de c√≥digo, que √© ent√£o convertida em uma lista Python e usada dinamicamente no par√¢metro options do widget.

Dessa forma, o formul√°rio se torna totalmente configur√°vel, permitindo f√°cil adi√ß√£o ou modifica√ß√£o das op√ß√µes dos dropdowns, como as do campo "comprimento", sem alterar o c√≥digo principal.
"""

opt_length = "Curto, M√©dio, Longo, 1 par√°grafo, 1 p√°gina" # @param {type:"string"}
print(opt_length)

options_length = [x.strip() for x in opt_length.split(",")]

options_length

length = widgets.Dropdown(
    options = opt_length,
    description="Tamanho",
    layout=widgets.Layout(width=w_dropdown)
)

form = create_form()
output.clear_output()
display(form)

"""---

## Constru√ß√£o de interface com Streamlit

Ap√≥s validar que nossa aplica√ß√£o est√° funcionando corretamente, podemos aprimorar ainda mais a interface.

Embora o uso de ipywidgets pode ser funcional, conseguimos criar uma experi√™ncia mais amig√°vel e visual com o **Streamlit** ‚Äî uma ferramenta focada em interfaces interativas para aplica√ß√µes em Python. Al√©m disso, o Streamlit facilita o deploy da aplica√ß√£o, tornando-a mais acess√≠vel para equipes de atendimento ou at√© mesmo clientes finais.

### 1. Instala√ß√£o do Streamlit

Para come√ßarmos, precisamos instalar o **Streamlit**

Por estarmos rodando no Colab, precisa tamb√©m instalar o **Localtunnel** para conseguirmos nos conectar √† aplica√ß√£o gerada com o streamlit. Ao executar em seu pr√≥prio computador ela n√£o √© necess√°ria, pois ap√≥s rodar o comando de launch do streamlit ("streamlit run ...") ser√° aberto automaticamente uma aba em seu navegador com a aplica√ß√£o.

Al√©m disso, vamos instalar a biblioteca **dotenv**, usada para simplificar a gest√£o de vari√°veis de ambiente ao armazen√°-las em um arquivo .env.
"""

!pip install -q streamlit
!npm install -q localtunnel
!pip install -q python-dotenv

"""### 2. Cria√ß√£o do arquivo da aplica√ß√£o

Crie um arquivo chamado `app.py` (ou outro nome que preferir) com o conte√∫do do seu c√≥digo adaptado para Streamlit.

Antes de colocarmos o c√≥digo nesse arquivo, vamos criar o arquivo .env, para carregar as vari√°veis de ambiente. Aqui basta colocarmos a key do Groq, a mesma que usamos anteriormente. Deixe nesse formato: `GROQ_API_KEY=CHAVE_AQUI`

* Obs: o comando `%%writefile` no in√≠cio desse bloco de c√≥digo permite que a c√©lula do notebook seja salva como um arquivo externo, com o nome especificado. Ou seja, estamos criando um arquivo com esse nome e o conte√∫do ser√° tudo a partir da segunda linha do bloco abaixo

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile .env
# GROQ_API_KEY=

"""Foi necess√°rio fazer algumas adapta√ß√µes ao c√≥digo, pois at√© ent√£o usamos ipywidgets mas agora no Streamlit usaremos fun√ß√µes da pr√≥pria biblioteca para criar os campos."""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain_groq import ChatGroq
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from dotenv import load_dotenv
# load_dotenv()
# 
# ## conex√£o com a LLM
# id_model = "llama3-70b-8192"
# llm = ChatGroq(
#     model=id_model,
#     temperature=0.7,
#     max_tokens=None,
#     timeout=None,
#     max_retries=2,
# )
# 
# ## fun√ß√£o de gera√ß√£o
# def llm_generate(llm, prompt):
#   template = ChatPromptTemplate.from_messages([
#       ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
#       ("human", "{prompt}"),
#   ])
# 
#   chain = template | llm | StrOutputParser()
# 
#   res = chain.invoke({"prompt": prompt})
#   return res
# 
# st.set_page_config(page_title = "Gerador de conte√∫do ü§ñ", page_icon="ü§ñ")
# st.title("Gerador de conte√∫do")
# 
# # Campos do formul√°rio
# topic = st.text_input("Tema:", placeholder="Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.")
# platform = st.selectbox("Plataforma:", ['Instagram', 'Facebook', 'LinkedIn', 'Blog', 'E-mail'])
# tone = st.selectbox("Tom:", ['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'])
# length = st.selectbox("Tamanho:", ['Curto', 'M√©dio', 'Longo'])
# audience = st.selectbox("P√∫blico-alvo:", ['Geral', 'Jovens adultos', 'Fam√≠lias', 'Idosos', 'Adolescentes'])
# cta = st.checkbox("Incluir CTA")
# hashtags = st.checkbox("Retornar Hashtags")
# keywords = st.text_area("Palavras-chave (SEO):", placeholder="Ex: bem-estar, medicina preventiva...")
# 
# if st.button("Gerar conte√∫do"):
#   prompt = f"""
#   Escreva um texto com SEO otimizado sobre o tema '{topic}'.
#   Retorne em sua resposta apenas o texto final e n√£o inclua ela dentro de aspas.
#   - Onde ser√° publicado: {platform}.
#   - Tom: {tone}.
#   - P√∫blico-alvo: {audience}.
#   - Comprimento: {length}.
#   - {"Inclua uma chamada para a√ß√£o clara." if cta else "N√£o inclua chamada para a√ß√£o"}
#   - {"Retorne ao final do texto hashtags relevantes." if hashtags else "N√£o inclua hashtags."}
#   {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords if keywords else ""}
#   """
#   try:
#       res = llm_generate(llm, prompt)
#       st.markdown(res)
#   except Exception as e:
#       st.error(f"Erro: {e}")

"""### 3. Execu√ß√£o do Streamlit

Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplica√ß√£o pelo streamlit.
Isso far√° com que a aplica√ß√£o do Streamlit seja executada em segundo plano.
"""

!streamlit run app.py &>/content/logs.txt &

"""> **Como abrir a interface**

> Importante: caso esse c√≥digo n√£o funcione corretamente use o ngrok, cujo c√≥digo voc√™ encontra mais abaixo (para mais detalhes, veja a aula 'Aviso sobre uso no Colab')

* Antes de conectar com o localtunnel, voc√™ precisa obter o IP externo (usando esse comando `!wget -q -O - ipv4.icanhazip.com`). Copie esse n√∫mero, que vai aparecer na sa√≠da do bloco abaixo (ap√≥s rodar)
* Ent√£o, entre no link que aparece na sa√≠da do bloco abaixo e informe esse IP no campo Tunnel Password. Logo em seguida, clique no bot√£o e aguarde a interface ser inicializada


Esse comando usa npx localtunnel para "expor" o aplicativo Streamlit em execu√ß√£o local para a internet. O aplicativo √© hospedado na porta 8501, e o localtunnel fornece uma URL p√∫blica por meio da qual o aplicativo pode ser acessado.

**Caso n√£o abra, reinicie a sess√£o e espere alguns segundos antes de clicar no link. Ou, reinicie o ambiente de execu√ß√£o e rode os comandos novamente.**
"""

!wget -q -O - ipv4.icanhazip.com
!npx localtunnel --port 8501

"""> **Importante:** Caso o comando acima com localtunnel n√£o funcione, use o c√≥digo abaixo (Para mais detalhes, consulte a aula "Aviso sobre uso no Colab" da se√ß√£o 2)

### Alternativa com ngrok
"""

!pip install pyngrok

from pyngrok import ngrok

!ngrok config add-authtoken SEU_TOKEN_AQUI
!streamlit run app.py --server.port 8501 &>/content/logs.txt &

public_url = ngrok.connect(8501)
public_url

"""---

## Rodando a LLM localmente

Se for um modelo open source n√≥s podemos fazer o download e rodar localmente em um provedor cloud (como nesse caso o colab) ou em nosso pr√≥prio computador.

### -> Para executar no Colab

**Importante:** Antes de realizar os pr√≥ximos passos, mude o ambiente de execu√ß√£o no Colab para usar GPU, que ser√° necess√°rio j√° que todo o processamento ser√° feito direto localmente no ambiente de execu√ß√£o do Colab. Para isso, selecione 'Ambiente de execu√ß√£o > Alterar o tipo de ambiente de execu√ß√£o' e na op√ß√£o 'Acelerador de hardware' selecione 'GPU'.

Al√©m das bibliotecas do langchain que instalamos, vamos precisar tamb√©m da biblioteca `langchain-huggingface`, `transformers` e `bitsandbytes`
"""

!pip install -q langchain langchain-community langchain-huggingface transformers

!pip install bitsandbytes-cuda110 bitsandbytes

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

"""**Quantiza√ß√£o**

A execu√ß√£o de LLMs pode ser desafiadora devido aos recursos limitados, especialmente na vers√£o gratuita do Google Colab. Para contornar essa limita√ß√£o, al√©m de escolher modelos com menos par√¢metros podemos usar t√©cnicas de quantiza√ß√£o, como o `BitsAndBytesConfig` da biblioteca `transformers`, que permitem carregar e executar modelos massivos de forma eficiente sem comprometer significativamente o desempenho.
* Essas t√©cnicas reduzem os custos de mem√≥ria e computa√ß√£o ao representar pesos e ativa√ß√µes com tipos de dados de menor precis√£o, como inteiros de 8 bits (int8) ou at√© 4 bits, tornando vi√°vel o uso de modelos grandes mesmo em hardware limitado.
* Alternativas ao BitsAndBytesConfig: AutoGPTQ, AutoAWQ, etc.
* Para quem prefere evitar configura√ß√µes complexas de otimiza√ß√£o e manter a m√°xima qualidade, considere o uso via API.
* Mais detalhes sobre quantiza√ß√£o: https://huggingface.co/blog/4bit-transformers-bitsandbytes
"""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

"""**Download do modelo**

Agora faremos o download e a configura√ß√£o de um modelo do HuggingFace usando o m√©todo `AutoModelForCausalLM.from_pretrained`. Este processo pode levar alguns minutos, pois o modelo tem alguns GB - mas no geral o download no Colab deve ser relativamente r√°pido.

> Para ver todos os modelos dispon√≠veis no Hugging Face, acesse: https://huggingface.co/models?pipeline_tag=text-generation

Escolhemos o Phi 3 (microsoft/Phi-3-mini-4k-instruct), um modelo menor mas que demonstrou ser muito interessante e com √≥timo custo benef√≠cio
 - https://huggingface.co/microsoft/Phi-3-mini-4k-instruct


"""

model_id = "microsoft/Phi-3-mini-4k-instruct"

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

"""**Cria√ß√£o do Pipeline**

Agora criaremos um pipeline para gera√ß√£o de texto usando nosso modelo e tokenizer carregados anteriormente. A fun√ß√£o de pipeline HuggingFace simplifica o processo de execu√ß√£o de v√°rias tarefas de processamento de linguagem natural ao fornecer uma interface de alto n√≠vel.

Par√¢metros:
* `model`: Modelo de linguagem a ser usado (definido por model_id).

* `tokenizer`: Tokenizador correspondente ao modelo para processar o texto.

* `task`: Tipo de tarefa (ex.: "text-generation" para gera√ß√£o de texto).

* `temperature`: Controla a aleatoriedade (lembre-se de variar o valor, conforme as dicas que passamos).

* `max_new_tokens`: N√∫mero m√°ximo de tokens gerados na sa√≠da.

* `do_sample`: Habilita/desabilita amostragem estoc√°stica (gera√ß√£o n√£o determin√≠stica).

* `repetition_penalty`: Penaliza repeti√ß√µes (valores >1 reduzem repeti√ß√µes).

* `return_full_text`: Se False, retorna apenas o texto gerado (ignorando o prompt).
"""

pipe = pipeline(
    model = model,
    tokenizer = tokenizer,
    task = "text-generation",
    temperature = 0.1,
    max_new_tokens = 500,
    do_sample = True,
    repetition_penalty = 1.1,
    return_full_text = False
)

"""Para carregar a LLM"""

llm = HuggingFacePipeline(pipeline = pipe)

input = "Gere um texto sobre alimenta√ß√£o saud√°vel, em 1 par√°grafo"

"""**Gera√ß√£o do resultado**"""

output = llm.invoke(input)
print(output)

"""**Adequando o prompt com Templates (quando necess√°rio)**

Talvez o resultado acima ficou um pouco estranho, ou ele inventou algum texto antes de fornecer o resultado.
Para evitar alucina√ß√µes ou gera√ß√£o infinita de texto, use o template oficial do modelo Phi 3, que inclui tokens especiais como:

* <|system|>, <|user|>, <|assistant|>: definem os pap√©is da mensagem.

* <|end|>: marca o fim do texto (equivalente ao token EOS).

Na d√∫vida, acesse a p√°gina do modelo no Hugging Face, se houver um template recomend√°vel para o modelo ele estar√° na descri√ß√£o.

Para outras implementa√ß√µes pode n√£o ser necess√°rio fornecer o prompt, como por exemplo a implementa√ß√£o via API que usamos anteriormente.


"""

prompt = """
<|system|>
Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva.<|end|>
<|user|>
"{}"<|end|>
<|assistant|>
""".format(input)

prompt

output = llm.invoke(prompt)
output

"""* Considera√ß√µes finais: A vantagem de usarmos o LangChain √© que toda a sintaxe e l√≥gca que criamos para esse projeto (por exemplo chains) √© reaproveitada, o que muda √© a parte de carregar a llm, o resto pode permanecer igual. Ent√£o, bastaria substituir o m√©todo de carregamento da LLM do LangChain (por exemplo, ao inv√©s de ChatGroq usar o HuggingFacePipeline ou o ChatHuggingFace) e com isso voc√™ teria a aplica√ß√£o funcionando o mesmo modo, por√©m rodando tudo localmente (seja cloud ou no computador local)

### -> Para rodar em seu computador

Para usar a LLM localmente via API: use o mesmo c√≥digo desse Colab, fazendo a instala√ß√£o das bibliotecas instaladas (no comando de instala√ß√£o, ao in√≠cio desse Colab).

Para usar a LLM baixando o modelo localmente:
Para maior compatibilidade de execu√ß√£o de LLMs em m√°quina local n√≥s sugerimos a biblioteca [Ollama](https://ollama.com), que possui integra√ß√£o direta com o LangChain.

* Rode o arquivo llm_local.py e instale todas as bibliotecas necess√°rias conforme consta nos coment√°rios ao in√≠cio do .py

Recomendamos usar pelo Colab pelo menos no in√≠cio e para n√£o atrapalhar o fluxo de aprendizado deste curso. Ao executar localmente podem ocorrer outros problemas de instala√ß√£o ou incompatibilidade, e de in√≠cio pode perder tempo desnecess√°rio. O m√©todo que mostraremos tenta evitar esses tipos de erro mas ainda assim √© imposs√≠vel garantir 100%, portanto sugerimos primeiro testar pelo Colab e depois (se quiser) executar em sua m√°quina local.
"""
